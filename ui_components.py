"""UI ì»´í¬ë„ŒíŠ¸ í•¨ìˆ˜ë“¤"""

import streamlit as st


def render_file_metadata_sidebar(file_meta):
    """ì‚¬ì´ë“œë°”ì— íŒŒì¼ ë©”íƒ€ë°ì´í„°ë¥¼ ë Œë”ë§í•©ë‹ˆë‹¤."""
    st.markdown(f"**íŒŒì¼ í¬ê¸°:** {file_meta['file_size_bytes']:,} bytes ({file_meta['file_size_mb']} MB)")
    st.markdown(f"**íŒŒì¼ íƒ€ì…:** `{file_meta['file_type']}`")

    if file_meta.get('character_count') and file_meta['character_count'] != "N/A (binary file)":
        st.markdown(f"**ë¬¸ì ìˆ˜:** {file_meta['character_count']:,}")
        st.markdown(f"**ë‹¨ì–´ ìˆ˜:** {file_meta['word_count']:,}")

    if isinstance(file_meta.get('estimated_tokens'), int):
        st.markdown(f"**ì¶”ì • í† í°:** ~{file_meta['estimated_tokens']:,}")
        st.markdown(f"**ì¶”ì • ì²­í¬:** ~{file_meta['estimated_chunks']}")

    st.markdown(f"**ì—…ë¡œë“œ ì‹œê°„:** {file_meta['upload_duration_seconds']}ì´ˆ")


def render_source_citations(chunks):
    """ê²€ìƒ‰ëœ ì¶œì²˜ë¥¼ ë Œë”ë§í•©ë‹ˆë‹¤."""
    with st.expander(f"ğŸ“š ê²€ìƒ‰ëœ ì¶œì²˜ ({len(chunks)}ê°œ)", expanded=False):
        for chunk in chunks:
            st.markdown(f"### ì¶œì²˜ {chunk['index']}")

            if "retrieved_context" in chunk:
                ctx = chunk["retrieved_context"]

                if "title" in ctx:
                    st.markdown(f"**ğŸ“Œ ì œëª©:** {ctx['title']}")
                if "uri" in ctx:
                    st.markdown(f"**ğŸ”— íŒŒì¼:** `{ctx['uri']}`")
                if "text" in ctx:
                    st.markdown("**ğŸ“ ì°¸ì¡° í…ìŠ¤íŠ¸:**")
                    text = ctx['text']
                    if len(text) > 500:
                        st.info(text[:500] + "...")
                        with st.expander("ì „ì²´ í…ìŠ¤íŠ¸ ë³´ê¸°"):
                            st.code(text, language=None)
                    else:
                        st.info(text)
            else:
                st.warning("ì¶œì²˜ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

            if chunk['index'] < len(chunks):
                st.divider()


def render_debug_info(debug_info):
    """ë””ë²„ê¹… ì •ë³´ë¥¼ ë Œë”ë§í•©ë‹ˆë‹¤."""
    with st.expander("ğŸ” ìƒì„¸ ë””ë²„ê¹… ì •ë³´ (ì „ì²´)", expanded=False):
        st.markdown("### Debug Info ì „ì²´ êµ¬ì¡°")
        st.json(debug_info)

    if debug_info.get("has_grounding"):
        with st.expander("ğŸ” Grounding ìƒì„¸ ì •ë³´", expanded=False):
            # Grounding Chunks í‘œì‹œ
            if debug_info.get("grounding_chunks"):
                st.markdown("## ğŸ“¦ ê²€ìƒ‰ëœ ë¬¸ì„œ ì²­í¬ (Grounding Chunks)")
                st.markdown("AIê°€ ë‹µë³€ì„ ìƒì„±í•  ë•Œ ì°¸ì¡°í•œ ë¬¸ì„œ ì¡°ê°ë“¤ì…ë‹ˆë‹¤.")

                for chunk in debug_info["grounding_chunks"]:
                    st.markdown(f"### Chunk {chunk['index']}")

                    if "retrieved_context" in chunk:
                        ctx = chunk["retrieved_context"]

                        if "title" in ctx:
                            st.markdown(f"**ğŸ“Œ ì œëª©:** {ctx['title']}")
                        if "uri" in ctx:
                            st.markdown(f"**ğŸ”— URI:** `{ctx['uri']}`")
                        if "text" in ctx:
                            st.markdown("**ğŸ“„ í…ìŠ¤íŠ¸ ë‚´ìš©:**")
                            st.code(ctx['text'], language=None)

                    st.divider()

            # Grounding Supports í‘œì‹œ
            if debug_info.get("grounding_supports"):
                st.markdown("## ğŸ¯ ë‹µë³€ ê·¼ê±° (Grounding Supports)")
                st.markdown("ë‹µë³€ì˜ ê° ë¶€ë¶„ì´ ì–´ë–¤ ë¬¸ì„œ ì²­í¬ë¥¼ ì°¸ì¡°í–ˆëŠ”ì§€ ë³´ì—¬ì¤ë‹ˆë‹¤.")

                for support in debug_info["grounding_supports"]:
                    st.markdown(f"### Support {support['index']}")

                    if "segment" in support:
                        seg = support["segment"]
                        st.markdown("**ğŸ“ ë‹µë³€ í…ìŠ¤íŠ¸:**")
                        st.info(seg.get("text", ""))
                        if seg.get("start_index") is not None:
                            st.markdown(f"**ìœ„ì¹˜:** {seg['start_index']} ~ {seg.get('end_index', 'N/A')}")

                    if "chunk_indices" in support:
                        st.markdown(f"**ğŸ“¦ ì°¸ì¡° ì²­í¬:** {support['chunk_indices']}")

                    if "confidence_scores" in support:
                        st.markdown(f"**ì‹ ë¢°ë„:** {support['confidence_scores']}")

                    st.divider()


def render_file_metadata_detail(file_metadata):
    """íŒŒì¼ ë©”íƒ€ë°ì´í„° ìƒì„¸ ì •ë³´ë¥¼ ë Œë”ë§í•©ë‹ˆë‹¤."""
    col1, col2, col3 = st.columns(3)

    with col1:
        st.metric("íŒŒì¼ í¬ê¸°", f"{file_metadata['file_size_mb']} MB")
        if isinstance(file_metadata.get('estimated_tokens'), int):
            st.metric("ì¶”ì • í† í°", f"{file_metadata['estimated_tokens']:,}")

    with col2:
        if file_metadata.get('character_count') and file_metadata['character_count'] != "N/A (binary file)":
            st.metric("ë¬¸ì ìˆ˜", f"{file_metadata['character_count']:,}")
        if file_metadata.get('word_count') != "N/A":
            st.metric("ë‹¨ì–´ ìˆ˜", f"{file_metadata['word_count']:,}")

    with col3:
        if isinstance(file_metadata.get('estimated_chunks'), int):
            st.metric("ì¶”ì • ì²­í¬", f"{file_metadata['estimated_chunks']}")
        st.metric("ì—…ë¡œë“œ ì‹œê°„", f"{file_metadata['upload_duration_seconds']}ì´ˆ")

    # ì²­í‚¹ ì„¤ì • í‘œì‹œ
    st.markdown("**âš™ï¸ ì²­í‚¹ ì„¤ì •:**")
    chunking = file_metadata['chunking_config']
    st.code(f"""
ì²­í‚¹ ë°©ì‹: {chunking['chunking_method']}
ìµœëŒ€ í† í°/ì²­í¬: {chunking['max_tokens_per_chunk']}
ì˜¤ë²„ë© í† í°: {chunking['max_overlap_tokens']}
    """.strip())

    # Operation ê²°ê³¼ (ê³ ê¸‰ ì •ë³´)
    if file_metadata.get('operation_result') or file_metadata.get('operation_metadata'):
        with st.expander("ğŸ” Gemini API ì‘ë‹µ (ê³ ê¸‰)"):
            if file_metadata.get('operation_result'):
                st.markdown("### Operation Result")
                st.json(file_metadata['operation_result'])

            if file_metadata.get('operation_metadata'):
                st.markdown("### Operation Metadata")
                st.json(file_metadata['operation_metadata'])


def render_example_questions():
    """ì˜ˆì‹œ ì§ˆë¬¸ì„ ë Œë”ë§í•©ë‹ˆë‹¤."""
    with st.expander("ğŸ“ ì§ˆë¬¸ ì˜ˆì‹œ ë³´ê¸°"):
        st.markdown("""
        - ì´ ë¬¸ì„œì˜ ì£¼ìš” ë‚´ìš©ì„ ìš”ì•½í•´ì£¼ì„¸ìš”
        - [íŠ¹ì • ì£¼ì œ]ì— ëŒ€í•´ ì„¤ëª…í•´ì£¼ì„¸ìš”
        - [í‚¤ì›Œë“œ]ê°€ ì–¸ê¸‰ëœ ë¶€ë¶„ì„ ì°¾ì•„ì£¼ì„¸ìš”
        - [ê°œë…A]ì™€ [ê°œë…B]ì˜ ì°¨ì´ì ì€ ë¬´ì—‡ì¸ê°€ìš”?
        - ë¬¸ì„œì—ì„œ ì¤‘ìš”í•œ ìˆ«ìë‚˜ í†µê³„ê°€ ìˆë‚˜ìš”?
        """)


def render_footer():
    """í‘¸í„°ë¥¼ ë Œë”ë§í•©ë‹ˆë‹¤."""
    st.divider()
    st.markdown(
        """
        <div style='text-align: center; color: #808080; font-size: 0.9em; padding: 1rem 0;'>
            <p>Powered by <strong>Google Gemini 2.5 Flash</strong> & <strong>Streamlit</strong></p>
            <p>ğŸ’¡ Tip: Ctrl+K to focus chat input | Ctrl+L to clear (ì‚¬ì´ë“œë°” ë²„íŠ¼ ì‚¬ìš©)</p>
        </div>
        """,
        unsafe_allow_html=True
    )
