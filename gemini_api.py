"""Gemini API ê´€ë ¨ í•¨ìˆ˜ë“¤"""

import os
import time
import uuid
from google import genai
from google.genai import types
from config import CHUNKING_CONFIG, MODEL_CONFIG, UPLOAD_CONFIG


def initialize_client():
    """í™˜ê²½ ë³€ìˆ˜ì—ì„œ API í‚¤ë¥¼ ë¡œë“œí•˜ê³  í´ë¼ì´ì–¸íŠ¸ë¥¼ ì´ˆê¸°í™”í•©ë‹ˆë‹¤."""
    try:
        api_key = os.getenv("GEMINI_API_KEY")
        if not api_key:
            return None, "GEMINI_API_KEYê°€ .env íŒŒì¼ì— ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."

        os.environ["GEMINI_API_KEY"] = api_key
        client = genai.Client()
        return client, None
    except Exception as e:
        return None, str(e)


def create_store(client, store_name):
    """File Search Storeë¥¼ ìƒì„±í•©ë‹ˆë‹¤."""
    try:
        store = client.file_search_stores.create(
            config={"display_name": store_name}
        )
        return store, None
    except Exception as e:
        return None, str(e)


def upload_file(client, file, store_name):
    """íŒŒì¼ì„ ì—…ë¡œë“œí•˜ê³  ì¸ë±ì‹±í•©ë‹ˆë‹¤."""
    try:
        # íŒŒì¼ ë©”íƒ€ë°ì´í„° ìˆ˜ì§‘
        file_metadata = {
            "filename": file.name,
            "file_size_bytes": file.size,
            "file_size_mb": round(file.size / (1024 * 1024), 2),
            "file_type": os.path.splitext(file.name)[1],
            "chunking_config": CHUNKING_CONFIG.copy()
        }

        # ì„ì‹œ íŒŒì¼ ìƒì„±
        file_ext = os.path.splitext(file.name)[1]
        temp_file = f"temp_{uuid.uuid4().hex}{file_ext}"

        file_content = file.getbuffer()
        with open(temp_file, "wb") as f:
            f.write(file_content)

        # í…ìŠ¤íŠ¸ íŒŒì¼ì¸ ê²½ìš° ë¬¸ì ìˆ˜ ê³„ì‚°
        if file_ext.lower() in UPLOAD_CONFIG['text_extensions']:
            try:
                text_content = file_content.tobytes().decode('utf-8')
                file_metadata["character_count"] = len(text_content)
                file_metadata["word_count"] = len(text_content.split())
                file_metadata["estimated_tokens"] = len(text_content) // 4
            except:
                file_metadata["character_count"] = "N/A (binary file)"
                file_metadata["word_count"] = "N/A"
                file_metadata["estimated_tokens"] = "N/A"
        else:
            file_metadata["character_count"] = "N/A (binary file)"
            file_metadata["word_count"] = "N/A"
            file_metadata["estimated_tokens"] = file.size // 4

        # íŒŒì¼ ì—…ë¡œë“œ
        start_time = time.time()
        operation = client.file_search_stores.upload_to_file_search_store(
            file=temp_file,
            file_search_store_name=store_name,
            config={
                "display_name": file.name,
                "chunking_config": {
                    "white_space_config": {
                        "max_tokens_per_chunk": CHUNKING_CONFIG["max_tokens_per_chunk"],
                        "max_overlap_tokens": CHUNKING_CONFIG["max_overlap_tokens"]
                    }
                }
            }
        )

        # ì—…ë¡œë“œ ì™„ë£Œ ëŒ€ê¸°
        while not operation.done:
            time.sleep(2)
            operation = client.operations.get(operation)

        file_metadata["upload_duration_seconds"] = round(time.time() - start_time, 2)

        # Operation ê²°ê³¼ ë©”íƒ€ë°ì´í„° ìˆ˜ì§‘
        if hasattr(operation, 'result'):
            result = operation.result
            file_metadata["operation_result"] = {}
            for attr in dir(result):
                if not attr.startswith('_'):
                    try:
                        value = getattr(result, attr)
                        if not callable(value):
                            file_metadata["operation_result"][attr] = str(value)
                    except:
                        pass

        if hasattr(operation, 'metadata'):
            metadata = operation.metadata
            file_metadata["operation_metadata"] = {}
            for attr in dir(metadata):
                if not attr.startswith('_'):
                    try:
                        value = getattr(metadata, attr)
                        if not callable(value):
                            file_metadata["operation_metadata"][attr] = str(value)
                    except:
                        pass

        # ì²­í¬ ê°œìˆ˜ ì¶”ì •
        if isinstance(file_metadata["estimated_tokens"], int):
            estimated_chunks = max(1, file_metadata["estimated_tokens"] // CHUNKING_CONFIG["max_tokens_per_chunk"])
            file_metadata["estimated_chunks"] = estimated_chunks
        else:
            file_metadata["estimated_chunks"] = "N/A"

        # ì„ì‹œ íŒŒì¼ ì •ë¦¬
        if os.path.exists(temp_file):
            os.remove(temp_file)

        return True, file_metadata, None

    except Exception as e:
        if 'temp_file' in locals() and os.path.exists(temp_file):
            os.remove(temp_file)
        return False, None, str(e)


def query_store(client, question, store_name):
    """Storeì— ì§ˆë¬¸í•˜ê³  ë‹µë³€ì„ ë°›ìŠµë‹ˆë‹¤."""
    try:
        response = client.models.generate_content(
            model=MODEL_CONFIG["model_name"],
            contents=question,
            config=types.GenerateContentConfig(
                tools=[
                    types.Tool(
                        file_search=types.FileSearch(
                            file_search_store_names=[store_name]
                        )
                    )
                ],
                temperature=MODEL_CONFIG["temperature"]
            )
        )

        # ë””ë²„ê¹… ì •ë³´ ìˆ˜ì§‘
        debug_info = {
            "has_grounding": False,
            "grounding_chunks": [],
            "grounding_supports": [],
            "citations": [],
            "raw_response_info": {}
        }

        citations = []

        # API ì‘ë‹µ êµ¬ì¡° í™•ì¸ì„ ìœ„í•œ ë¡œê¹…
        print("\n" + "="*80)
        print("ğŸ” Gemini API Response Debug")
        print("="*80)

        # response ê°ì²´ì˜ ëª¨ë“  ì†ì„± í™•ì¸
        print("\nğŸ“¦ Response ê°ì²´ ì†ì„±:")
        for attr in dir(response):
            if not attr.startswith('_'):
                try:
                    value = getattr(response, attr)
                    if not callable(value):
                        print(f"  - {attr}: {type(value).__name__}")
                except:
                    pass

        # automatic_function_calling_history í™•ì¸
        if hasattr(response, 'automatic_function_calling_history') and response.automatic_function_calling_history:
            print(f"\nğŸ“œ automatic_function_calling_history ë°œê²¬! ê°œìˆ˜: {len(response.automatic_function_calling_history)}")
            for idx, history_item in enumerate(response.automatic_function_calling_history):
                print(f"\n  History {idx}:")
                for attr in dir(history_item):
                    if not attr.startswith('_'):
                        try:
                            value = getattr(history_item, attr)
                            if not callable(value):
                                print(f"    - {attr}: {type(value).__name__}")
                        except:
                            pass

        # parts í™•ì¸
        if hasattr(response, 'parts') and response.parts:
            print(f"\nğŸ“„ response.parts ë°œê²¬! ê°œìˆ˜: {len(response.parts)}")
            for idx, part in enumerate(response.parts):
                print(f"\n  Part {idx}:")
                for attr in dir(part):
                    if not attr.startswith('_'):
                        try:
                            value = getattr(part, attr)
                            if not callable(value):
                                print(f"    - {attr}: {type(value).__name__}")
                        except:
                            pass

        # candidates í™•ì¸
        grounding_metadata = None
        if hasattr(response, 'candidates') and response.candidates:
            print(f"\nâœ… candidates ë°œê²¬! ê°œìˆ˜: {len(response.candidates)}")

            for idx, candidate in enumerate(response.candidates):
                print(f"\n  Candidate {idx}:")
                for attr in dir(candidate):
                    if not attr.startswith('_'):
                        try:
                            value = getattr(candidate, attr)
                            if not callable(value):
                                print(f"    - {attr}: {type(value).__name__}")
                                if attr == "grounding_metadata" and value:
                                    grounding_metadata = value
                                    print(f"      âœ… grounding_metadata ë°œê²¬!")
                        except:
                            pass

                # candidate.content í™•ì¸
                if hasattr(candidate, 'content') and candidate.content:
                    print(f"\n  Candidate {idx} Content:")
                    for attr in dir(candidate.content):
                        if not attr.startswith('_'):
                            try:
                                value = getattr(candidate.content, attr)
                                if not callable(value):
                                    print(f"    - {attr}: {type(value).__name__}")
                            except:
                                pass

        # grounding_metadata ì²˜ë¦¬ (response ì§ì ‘ ë˜ëŠ” candidates[0]ì—ì„œ)
        if not grounding_metadata and hasattr(response, "grounding_metadata"):
            grounding_metadata = response.grounding_metadata

        if grounding_metadata:
            debug_info["has_grounding"] = True
            print("\nâœ… grounding_metadata ì¡´ì¬!")
            print(f"  íƒ€ì…: {type(grounding_metadata)}")

            # grounding_metadataì˜ ëª¨ë“  ì†ì„± í™•ì¸
            print("\nğŸ“‹ grounding_metadata ì†ì„±:")
            for attr in dir(grounding_metadata):
                if not attr.startswith('_'):
                    try:
                        value = getattr(grounding_metadata, attr)
                        if not callable(value):
                            print(f"  - {attr}: {type(value).__name__}")
                            if hasattr(value, '__len__') and not isinstance(value, str):
                                try:
                                    print(f"    (ê¸¸ì´: {len(value)})")
                                except:
                                    pass
                    except:
                        pass

            # grounding_chunks ìˆ˜ì§‘ ë° citationsë¡œ ë³€í™˜
            if hasattr(grounding_metadata, "grounding_chunks"):
                chunks_list = grounding_metadata.grounding_chunks
                print(f"\nğŸ“¦ grounding_chunks ë°œê²¬! ê°œìˆ˜: {len(list(chunks_list)) if chunks_list else 0}")

                for idx, chunk in enumerate(grounding_metadata.grounding_chunks, 1):
                    print(f"\n  Chunk {idx}:")

                    # chunkì˜ ëª¨ë“  ì†ì„± í™•ì¸
                    for attr in dir(chunk):
                        if not attr.startswith('_'):
                            try:
                                value = getattr(chunk, attr)
                                if not callable(value):
                                    print(f"    - {attr}: {type(value).__name__}")
                            except:
                                pass
                    chunk_data = {"index": idx}

                    if hasattr(chunk, "web") and chunk.web:
                        chunk_data["web"] = str(chunk.web)

                    if hasattr(chunk, "retrieved_context") and chunk.retrieved_context:
                        ctx = chunk.retrieved_context
                        chunk_data["retrieved_context"] = {}
                        citation_item = {}

                        # retrieved_contextì˜ ëª¨ë“  ì†ì„± í™•ì¸
                        print(f"\n    ğŸ” Retrieved Context {idx} ì†ì„±:")
                        for attr in dir(ctx):
                            if not attr.startswith('_'):
                                try:
                                    value = getattr(ctx, attr)
                                    if not callable(value):
                                        print(f"      - {attr}: {type(value).__name__} = {repr(value)[:100]}")
                                except:
                                    pass

                        if hasattr(ctx, "title"):
                            chunk_data["retrieved_context"]["title"] = ctx.title
                            citation_item["title"] = ctx.title

                        if hasattr(ctx, "uri"):
                            chunk_data["retrieved_context"]["uri"] = ctx.uri
                            citation_item["uri"] = ctx.uri
                            citation_item["source"] = ctx.uri

                        if hasattr(ctx, "text"):
                            chunk_data["retrieved_context"]["text"] = ctx.text
                            citation_item["text"] = ctx.text

                        if citation_item:
                            citations.append(citation_item)

                    debug_info["grounding_chunks"].append(chunk_data)

                print(f"\nâœ… ì´ {len(debug_info['grounding_chunks'])}ê°œ chunks ìˆ˜ì§‘ ì™„ë£Œ")
            else:
                print("\nâŒ grounding_chunks ì†ì„±ì´ ì—†ìŠµë‹ˆë‹¤!")

            # grounding_supports ìˆ˜ì§‘
            if hasattr(grounding_metadata, "grounding_supports"):
                for idx, support in enumerate(grounding_metadata.grounding_supports, 1):
                    support_data = {"index": idx}

                    if hasattr(support, "segment"):
                        seg = support.segment
                        support_data["segment"] = {
                            "text": getattr(seg, "text", ""),
                            "start_index": getattr(seg, "start_index", None),
                            "end_index": getattr(seg, "end_index", None)
                        }

                    if hasattr(support, "grounding_chunk_indices") and support.grounding_chunk_indices is not None:
                        support_data["chunk_indices"] = list(support.grounding_chunk_indices)

                    if hasattr(support, "confidence_scores") and support.confidence_scores is not None:
                        support_data["confidence_scores"] = list(support.confidence_scores)

                    debug_info["grounding_supports"].append(support_data)

            # citations ìˆ˜ì§‘
            if hasattr(grounding_metadata, "citations"):
                for idx, citation in enumerate(grounding_metadata.citations, 1):
                    citation_data = {}
                    for attr in dir(citation):
                        if not attr.startswith('_'):
                            try:
                                value = getattr(citation, attr)
                                if not callable(value):
                                    citation_data[attr] = value
                            except:
                                pass
                    citations.append(citation_data)
                    debug_info["citations"].append(citation_data)
        else:
            print("\nâŒ grounding_metadataê°€ ì—†ìŠµë‹ˆë‹¤!")
            debug_info["raw_response_info"]["has_grounding_metadata"] = False

        print("\n" + "="*80)
        print(f"ğŸ“Š ìµœì¢… ìˆ˜ì§‘ ê²°ê³¼:")
        print(f"  - has_grounding: {debug_info['has_grounding']}")
        print(f"  - grounding_chunks: {len(debug_info['grounding_chunks'])}ê°œ")
        print(f"  - grounding_supports: {len(debug_info['grounding_supports'])}ê°œ")
        print(f"  - citations: {len(debug_info['citations'])}ê°œ")
        print("="*80 + "\n")

        return response.text, citations, debug_info, None

    except Exception as e:
        import traceback
        traceback.print_exc()
        return None, None, None, str(e)
